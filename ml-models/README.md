
# Medical-Bot

A small RAG (retrieval-augmented generation) medical chatbot built with Flask, LangChain, Pinecone and OpenAI-style chat models. The app indexes a medical PDF (stored under `data/`) into a Pinecone vector index and exposes a simple web chat UI at `/` that answers user questions using retrieved context.

## Repository layout

- `app.py` - Flask app and main web server. Loads embeddings + Pinecone vector store, wires up a retrieval chain and serves the chat UI.
- `store_index.py` - Script that loads PDFs from `data/`, splits text into chunks, creates/initializes a Pinecone index (`medical-chatbot`) and upserts embeddings.
- `src/helper.py` - Helpers for loading PDFs, splitting text and creating Hugging Face embeddings.
- `src/prompt.py` - System prompt used by the chat model.
- `templates/chat.html` - Frontend chat UI.
- `static/style.css` - Styling for the UI.
- `data/Medical_book.pdf` - Example PDF used as the knowledge source.
- `requirements.txt` - Python dependencies.

## Quick overview / contract

- Input: user text typed in the web UI.
- Output: concise, context-aware answer (up to 3 sentences) generated by the model using retrieved PDF context.
- Success: app starts, Pinecone index exists and contains embeddings, chat UI returns helpful answers.
- Errors: missing env vars (PINECONE_API_KEY, OPENAI_API_KEY), Pinecone index not created, model/API rate limiting or misconfiguration.

## Requirements

- Python 3.10+ (tested with 3.11)
- Pinecone account + API key
- OpenAI-compatible API key (the code expects an OpenAI-style Chat model via the `langchain_openai` package)

Install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate    # on Windows (Git Bash) you can also use: source .venv/Scripts/activate
pip install -r requirements.txt
```

Note: the project uses a Hugging Face sentence-transformers model for embedding (`sentence-transformers/all-MiniLM-L6-v2`) and a number of LangChain helper packages listed in `requirements.txt`.

## Environment variables

Create a `.env` file in the project root with the following values (do NOT commit this file):

```
PINECONE_API_KEY=your-pinecone-api-key
OPENAI_API_KEY=your-openai-api-key
```

The code loads these keys via python-dotenv and sets them on the environment before calling Pinecone/embedding APIs.

## Indexing the PDF (create & populate the Pinecone index)

Before running the web app you must create and populate the Pinecone index using `store_index.py`.

This script does the following:
- loads PDFs from `data/` (glob `*.pdf`)
- converts to `langchain.schema.Document` objects with only `source` in metadata
- splits text into chunks (chunk size 500, overlap 20)
- creates a Pinecone index named `medical-chatbot` (dimension 384, cosine metric)
- embeds and upserts the document chunks

Run the script:

```bash
python store_index.py
```

If the index already exists, the script will skip the creation step and upsert documents into the existing `medical-chatbot` index. You can change the index name in the script if needed.

## Running the web app (development)

Start the Flask app which serves `templates/chat.html` and an endpoint `/get` for chat requests:

```bash
python app.py
```

Open your browser to: http://localhost:5050

The frontend sends the user's message to `/get`, the server runs the retrieval-augmented generation (RAG) chain and returns a concise answer.

## Docker (optional)

This repository contains a `Dockerfile`. Quick build and run commands:

```bash
docker build -t medical-bot .
docker run -p 5050:5050 --env-file .env medical-bot
```

Ensure your `.env` values are available to the container (using `--env-file` or explicit `-e` flags).

## Configuration notes

- Index name: `medical-chatbot` (see `app.py` and `store_index.py`). Change carefully and re-run `store_index.py` to reindex.
- Embedding model: `sentence-transformers/all-MiniLM-L6-v2` (dimension 384). You can change to another HF model in `src/helper.py` but ensure dimension matches the Pinecone index.
- Chat model: `chatModel = ChatOpenAI(model="gpt-4o")` in `app.py`. Replace with a model name supported by your `langchain_openai` provider or your organization's endpoint.

## Troubleshooting

- Missing API keys / auth errors: check `.env` and that `PINECONE_API_KEY` and `OPENAI_API_KEY` are set and valid.
- Pinecone index errors: verify the index name and region in your Pinecone console. The indexing script uses ServerlessSpec with `cloud="aws"` and `region="us-east-1"`; change if your org uses a different configuration.
- Slow embedding downloads: Hugging Face models may be cached on first run; allow time for the model to download.
- CORS / networking: when running inside Docker, ensure the container has network access to Pinecone and OpenAI.

## Development & testing tips

- The UI is in `templates/chat.html` and makes a POST to `/get` with `msg` form data.
- You can call the chain directly in Python (e.g., from a REPL) by importing `app.rag_chain` if you need to run programmatic queries.
- Add more documents to `data/` (PDFs) and re-run `store_index.py` to expand knowledge.

## Security

- Keep your `.env` and API keys out of version control.
- Be mindful of returning sensitive medical advice. This project is a demo and should not be used as a substitute for professional medical guidance.

## Acknowledgements

- Built with Flask, LangChain, Pinecone, Hugging Face sentence-transformers and OpenAI-style chat models.
